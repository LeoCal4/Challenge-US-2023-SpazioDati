{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trafilatura\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaziodati_path = r\".\\spaziodati_sample.jsonl\"\n",
    "webpages_df = pd.read_json(path_or_buf=spaziodati_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {webpages_df.shape}\")\n",
    "print(f\"Columns: {webpages_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpages_df[\"status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpages_df = webpages_df.drop([\"status\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 25\n",
    "print(webpages_df.iloc[INDEX][\"url\"])\n",
    "print(trafilatura.extract(webpages_df.iloc[INDEX][\"content\"]))\n",
    "print(webpages_df.iloc[INDEX][\"people\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"it_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_entry = nlp(trafilatura.extract(webpages_df.iloc[INDEX][\"content\"], favor_recall=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(parsed_entry, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = set()\n",
    "for entry in webpages_df.itertuples(index=False):\n",
    "    for person in entry[1]:\n",
    "        roles.add(person[\"role\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Roles found: {len(roles)}\")\n",
    "roles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaziodati_path = r\".\\spaziodati_sample.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpages_df = pd.read_json(path_or_buf=spaziodati_path, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using BeautifulSoup\n",
    "This approach extract every visible text on the screen whatsoever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_pages = {}\n",
    "for i, entry in webpages_df.iterrows():\n",
    "    if i >= 100:\n",
    "        break\n",
    "    soup = bs4.BeautifulSoup(entry[\"content\"], 'html.parser')\n",
    "    page_text = soup.get_text().strip()\n",
    "    cleaned_page_text = re.sub(r\"^\\n$\", \"\", page_text) # remove newline only lines\n",
    "    cleaned_page_text = re.sub(r\"\\n{2,}\", \"\\n\", cleaned_page_text) # remove multiple newlines\n",
    "    cleaned_page_text = re.sub(r\"\\s{2,}\", \" \", cleaned_page_text) # remove multiple whitespaces\n",
    "    cleaned_page_text = cleaned_page_text.replace(\"\\n\", \" . \") # replace remaining newlines\n",
    "    cleaned_page_text = re.sub(r\"[^\\w\\s\\d\\.,:;’'@\\(\\)&]\", \" \", cleaned_page_text) # remove non conventional characters\n",
    "    cleaned_page_text = re.sub(r\"\\d{5,}\", \"[NUM]\", cleaned_page_text) # mask long numbers\n",
    "    cleaned_page_text = re.sub(r\"(?:\\.\\s+){2,}\", \". \", cleaned_page_text) # remove repeating periods followed by spaces\n",
    "    cleaned_page_text = re.sub(r\"\\b[A-Z]+(?:\\s+[A-Z]+)*\\b\", lambda pattern: pattern.group(0).title(), cleaned_page_text) # titolize full caps words\n",
    "    full_text_pages[entry[\"url\"]] = cleaned_page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"beautiful_soup_pages.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(full_text_pages, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Trafilatura [old approach]\n",
    "First approach used to extract text. The library only extract part of the text, the one that it deems \"important\" using its heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpages_text = webpages_df[\"content\"].loc[:100].apply(lambda text: trafilatura.extract(text, favor_recall=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = webpages_text.str.replace(\"\\n\", \" . \").\\\n",
    "    str.replace(r\"[^\\w\\s\\d\\.,:;’'@\\(\\)&]\", \" \").\\\n",
    "    str.replace(r\"\\s{2,}\", \" \").\\\n",
    "    str.replace(r\"\\d{5,}\", \"[NUM]\").\\\n",
    "    str.replace(r\"(?:\\.\\s+){2,}\", \". \").\\\n",
    "    str.strip(\" .\").\\\n",
    "    str.replace(r\"\\b[A-Z]+(?:\\s+[A-Z]+)*\\b\", lambda pattern: pattern.group(0).title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi . Un team vincente . Drago Press è un’agenzia di comunicazione, marketing, web e digital strategy, fondata negli anni Novanta, con sede a Padova e uffici di rappresentanza a Londra, Milano, Bologna e Roma. In grado di operare su scala nazionale e internazionale, l’agenzia è composta da un team di professionisti dinamici e propositivi, capaci di assistere efficacemente strutture che operano nei settori: tourism, food&beverage, luxury, medical, wellness, hotellerie. Maurizio Drago . Ha iniziato la sua attività di giornalista da quando frequentava la facoltà di Scienze Politiche all’Università di Padova. Ha proseguito la sua professione come dirigente presso le associazioni economiche. Poi si è buttato a capofitto nel lavoro di giornalista nella ricerca e valorizzazione dei prodotti tipici e dei luoghi caratteristici della nostra bella Italia, coniando per primo il nome di giornalista enogastroturista , acquisendo conoscenze specifiche nel settore del turismo e dei prodotti, della ricettività e della ristorazione. Amministratore unico di DragoPress, è autore di numerose guide al turismo enogastronomico e responsabile ufficio stampa di importanti fiere e Borse del Turismo. Direttore responsabile del quotidiano online Estensione.org, collabora con testate giornalistiche nazionali, Rai e tv private. Collaborazioni e pubblicazioni . Quotidiano Estensione, Il Gusto Italiano, Sapori d’Italia, Vino&Cibo, DiTUTTO (responsabile della rubrica Itinerari di viaggio ), Settimanale In Famiglia, Viaggi&Gusti.it, Con i piedi per terra, Sapori d’Italia, enogastronomia.info. Federica Pagliarone . Abruzzese di nascita, bolognese d’adozione, con una laurea in Lettere Moderne dopo aver mosso i primi passi come cronista per il gruppo Poligrafici Editoriale, ha avuto molteplici esperienze nell’editoria, specializzandosi nei settori: food, medical, wellness, tourism, hotellerie, luxury, design. Coautrice della Guida al turismo enogastronomico italiano Itinerari del gusto, prodotti e piatti tipici della tradizione italiana, scrive per i maggiori periodici nazionali di salute, viaggi e femminili, realizzando inchieste, interviste e reportage. Responsabile Ufficio Stampa per aziende, enti e strutture medicali, è specializzata nell’organizzazione di Press Tour nazionali e internazionali per progetti di marketing e promozione territoriale. Collaborazioni e pubblicazioni . Più Sani Più Belli, Alimentazione Naturale, Vanity Fair, Top Salute, Ok Salute, Cortina Magazine (responsabile della rubrica Noblesse Oblige ), Mete d’Italia e del Mondo, Viaggivacanze.info, VinoeCibo, DiTUTTO (responsabile della rubrica Itinerari di viaggio ), Corrierequotidiano.it, Viaggi&Gusti.it, Turisti per Caso, Turismo all’aria aperta, Degusta, Dieta Social, Terreincognitemagazine, Salute 10 . Alessia Crivellaro . Con una laurea magistrale in Editoria e Giornalismo, dopo la collaborazione con la redazione telegiornalistica dell’emittente Telepadova 7Gold, è stata redattrice di cronaca e attualità presso la testata giornalistica Estensione. Con esperienze di organizzazione e gestione di eventi a promozione sociale e dibattiti politici, collabora con il periodico bimestrale Con i Piedi per Terra e si occupa di attività d’ufficio stampa, rassegne stampa, organizzazione Press Day ed Educational Tour Press nella redazione DragoPress. Collaborazioni e pubblicazioni . Quotidiano Estensione, Euganeamente, Corriere Quotidiano, Con i piedi per terra. Martina Toso . Dopo una laurea magistrale in Editoria e Giornalismo, ha iniziato la collaborazione con DragoPress. Nel corso degli anni si è occupata di attività di ufficio stampa tradizionale, per poi orientarsi al mondo del digitale seguendo un master breve dedicato e corsi di aggiornamento. Si dedica alla scrittura e ottimizzazione di contenuti in ottica Seo per il web e di social media marketing, gestendo profili e pagine di realtà del territorio. Attualmente collabora con il periodico bimestrale Con i piedi per terra e con il quotidiano online Estensione. Collaborazioni e pubblicazioni . Quotidiano Estensione, Euganeamente, Corriere Quotidiano, Con i piedi per terra. Lorenzo Drago . Giornalista pubblicista dal 1993, lavorando nel settore della comunicazione e nel cinema, conosce i principali sistemici informatici e di elevata tecnologia. Dopo la carta stampata, ha iniziato a lavorare come cameraman per alcune tv locali e successivamente ha collaborato per svariati services con testate televisive di portata nazionale. Ha iniziato poi a intraprendere l’attività di videomaker e regista svolgendo reportage di cinema, cultura e viaggi anche fuori dai confini nazionali. A tal proposito possiede un canale YouTube, dove sono contenuti alcuni lavori. Realizza inoltre video promozionali, spot e documentari per conto di enti pubblici e privati. Collaborazioni e pubblicazioni . Mattino di Padova, Gazzettino di Venezia, Telesanterno, Telecentro, Odeont Tv, Autori Multimediali, New Time, Professional Fly Television, Fonoprint, Target Video, Mediterranean Tourism. Lamberto Mazzotti . Nato a Russi di Romagna, inizia la carriera giornalistica, battendo su una mitica Lettera 32, resoconti e cronache per famigliari e conoscenti. Conseguita la laurea, la passione per la comunicazione lo accompagna in 30 anni di giornalismo attivo nella veste di direttore dell’Agenzia di Comunicazione Centro Stampa di Bologna. Nel 1981 ha redatto un ampio reportage sui suini di razza Mora Romagnola contribuendo alla salvaguardia di questi esemplari in collaborazione con il Dipartimento di Scienze Zootecniche dell’Università di Torino. Nel 2015 il Presidente della Repubblica lo ha insignito del titolo di Cavaliere al Merito della Repubblica Italiana. Specialista dell’informazione agro alimentare, dall’aprile 2017 si occupa di comunicazione e cultura del cibo per importanti soggetti imprenditoriali, redige articoli e reportage per magazine di gastronomia e turismo. Collaborazioni e pubblicazioni . Terra e vita, Ortofrutta notizie, Vino & Cibo, Gustando, Gustando magazine, Agricoltura, Laguna. Giulia Schiavon . Con una laurea in Scienze della Comunicazione e un master in Marketing e Customer Management lavora coltivando le sue più grandi passioni: il food & beverage e il turismo enogastronomico. Dapprima responsabile della gestione di un’enoteca poi responsabile gastronomia, vendite e selezione vini per un’importante realtà del piovese, partecipa a fiere, degustazioni e masterclass. Co founder di un progetto turistico per la promozione del territorio e delle eccellenze enogastronomiche locali, ha conseguito l’attestato di Sommelier Professionista frequentando i tre corsi promossi dall’Ais (Associazione Italiana Sommelier). All’interno dell’agenzia DragoPress si occupa di Marketing e Comunicazione del vino, mettendo a frutto la sua padronanza del tessuto locale, di vini nazionali e internazionali. Michele Pigozzo . Narratore del gusto, una passione che lo porta a trasformare la sua percezione in un’emozione che gli permette di descrivere le sensazioni provate durante il suo viaggio sensoriale, narrandone i saporI, le sensazioni e i ricordi che affiorano o che si imprimono nella sua mente. Scrive e racconta di gusto e di sapore in forma narrativa ed emozionale. Ideatore del format televisivo Racconti di cibo e tradizioni. I suoi racconti si intrecciano alle tradizioni e ai detti popolari traducendosi spesso in leggende. Dedica al gusto pagine su varie riviste locali, racconta di cuochi e di ricette che riportano alla cultura del nostro territorio. Ai ragazzi dell’istituto professionale alberghiero racconta i segreti di come interpretare e raccontare i sapori. Collaborazioni e pubblicazioni . Con i piedi per terra, Area3 News, Cafe24, Venezia&dintorni il gusto italiano nel mondo. Autore dei libri: Quaderno dei sapori Berici, Low Carb high Health, Few Carb more Health. Partners . Estensione . Il giornale online della bassa padovana: notizie, approfondimenti, foto e video di cronaca, politica, cultura ed eventi. Con i piedi per terra . La rivista bimestrale di conoscenza del territorio con attualità e informazioni riguardanti la terra ed i suoi frutti. Offline agency . Una giovane realtà che fornisce tutti gli strumenti necessari per essere competitivi nel mercato con la comunicazione online e offline. InternationalCom . L’agenzia di comunicazione integrata che si occupa di ufficio stampa, E commerce, comunicazione web e mobile per offrire visibilità ad aziende italiane in Europa, Nord America, Russia e Cina. Starlight Video Production . L’azienda, con sede a Venezia e a Berlino, che si occupa di produzioni video come pubblicità, film, redazionali, documentari, eventi sportivi e non, video aziendali e videoclip musicali. Le testate giornalistiche con cui lavoriamo: . Il Sole 24 Ore, Il Corriere della Sera, Style, Repubblica, Rai News 24, Cortina Magazine, Vanity Fair, Cucina Italiana, Diva e Donna, Bell’Italia, Bell’Europa, In famiglia, Di tutto, Più Sani e Più Belli, Vino e Cibo, Food & Beverage, TidPress, Degusta, I viaggi del Gusto, Atmosphere, PfgStyle, Mete d’Italia e del Mondo, Il Mattino di Padova, Il Gazzettino, Il Corriere Quotidiano, La Voce di Rovigo, Il Giornale dell’Umbria, Il Giorno, Alimentazione naturale, Dieta social, Aurum, Sapori d’Italia, Le vie del gusto, Dove, Salute 10 , Viaggi&Gusti, Terre incognite magazine, Il Giornale di Vicenza, Viaggivacanze.info, Agronotizie, Estensione, Il mio medico (TvSAT2000), Sereno Variabile, Geo&Geo,Tgcom24, La salute vien mangiando (Alice Tv)\n"
     ]
    }
   ],
   "source": [
    "print(test_df[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_base_df = {url: text for url, text in zip(webpages_df.url.values, test_df.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trafilatura_pages.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_base_df, f, ensure_ascii=False, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"INSERT-KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaziodati_path = r\".\\beautiful_soup_pages.json\"\n",
    "with open(spaziodati_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    webpages_text = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_and_jobs_prompt_3_5 = \"\"\"Estrai tutti i nomi propri delle persone dal seguente testo e le rispettive posizioni lavorative.\n",
    "Il testo in questione è stato estratto da una pagina di contatti del sito web di un'azienda. Le informazioni da estrarre sono necessarie a creare una lista di persone che lavorano per l'azienda.\n",
    "Se la posizione lavorativa di una persona non è chiara o non è riportata, scrivi solo \"?\".\n",
    "La tua risposta deve essere formata unicamente dalla lista dei nomi delle persone, seguita da \":\" e poi dalla posizione lavorativa. Ad esempio: \"Mario Rossi: CEO\" o \"Luigi Verdi: ?\".\n",
    "Rispondi solo con la lista di nomi e ruoli lavorativi, non aggiungere commenti o altro.\n",
    "Di seguito riporto un esempio per aiutarti nell'estrazione di informazioni dal testo reale.\n",
    "TESTO:\n",
    "Nel 2011 alla guida della Camera è eletto Antonio Barile. . Il nuovo Presidente con entusiasmo ha dichiarato di voler consolidare le posizioni acquisite dalla Camera di Commercio Italo Orientale in questi anni, favorendo in tutti i contesti locali, nazionali ed internazionali, le azioni che possano permettere di collaborare e concretizzare progetti di media e lunga durata sia con le istituzioni pubbliche che private. . Già dopo pochi mesi dalla sua elezione ha siglato un interessante protocollo d’intesa con la Camera di Commercio di Prahova e a seguire con Halal Italy e con la Samer di Bari. Vitandrea Marzano . Vice Presidente. Annarita Torino . Tesoriere . Barnaba Alessandra, Demarinis Domenico, Laforgia Mario. Consiglieri. Lattanzio Michelangelo . Revisore (Presidente) . Boleto Carmela, Panza Massimo . Revisori\n",
    "OUTPUT:\n",
    "- Antonio Barile: presidente\n",
    "- Vitandrea Marzano: vice presidente\n",
    "- Annarita Torino: tesoriere\n",
    "- Alessandra Barnaba: consigliere\n",
    "- Domenico Demarinis: consigliere\n",
    "- Mario Laforgia: consigliere\n",
    "- Michelangelo Lattanzio: revisore (presidente)\n",
    "- Carmela Boleto: revisore\n",
    "- Massimo Panza: revisore\n",
    "TESTO:\n",
    "{}\n",
    "OUTPUT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_people_and_jobs_prompt_4 = \"\"\"Sei un analizzatore testuale di contenuti di pagine web.\n",
    "Estrai tutti i nomi propri delle persone dal testo che ti verrà fornito e le rispettive posizioni lavorative.\n",
    "Il testo in questione è stato estratto da una pagina di contatti del sito web di un'azienda. Le informazioni da estrarre sono necessarie a creare una lista di persone che lavorano per l'azienda.\n",
    "Se la posizione lavorativa di una persona non è chiara o non è riportata, scrivi solo \"?\".\n",
    "La tua risposta deve essere formata unicamente dalla lista dei nomi delle persone, seguita da \":\" e poi dalla posizione lavorativa. Ad esempio: \"Mario Rossi: CEO\" o \"Luigi Verdi: ?\".\n",
    "Rispondi solo con la lista di nomi e ruoli lavorativi, non aggiungere commenti o altro.\n",
    "Di seguito riporto un esempio per aiutarti nell'estrazione di informazioni dal testo reale.\n",
    "TESTO:\n",
    "Nel 2011 alla guida della Camera è eletto Antonio Barile. . Il nuovo Presidente con entusiasmo ha dichiarato di voler consolidare le posizioni acquisite dalla Camera di Commercio Italo Orientale in questi anni, favorendo in tutti i contesti locali, nazionali ed internazionali, le azioni che possano permettere di collaborare e concretizzare progetti di media e lunga durata sia con le istituzioni pubbliche che private. . Già dopo pochi mesi dalla sua elezione ha siglato un interessante protocollo d’intesa con la Camera di Commercio di Prahova e a seguire con Halal Italy e con la Samer di Bari. Vitandrea Marzano . Vice Presidente. Annarita Torino . Tesoriere . Barnaba Alessandra, Demarinis Domenico, Laforgia Mario. Consiglieri. Lattanzio Michelangelo . Revisore (Presidente) . Boleto Carmela, Panza Massimo . Revisori\n",
    "OUTPUT:\n",
    "- Antonio Barile: presidente\n",
    "- Vitandrea Marzano: vice presidente\n",
    "- Annarita Torino: tesoriere\n",
    "- Alessandra Barnaba: consigliere\n",
    "- Domenico Demarinis: consigliere\n",
    "- Mario Laforgia: consigliere\n",
    "- Michelangelo Lattanzio: revisore (presidente)\n",
    "- Carmela Boleto: revisore\n",
    "- Massimo Panza: revisore\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4\"\n",
    "\n",
    "def extract_names_and_roles(text: str) -> str:\n",
    "    if MODEL == \"gpt-3.5-turbo\":\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": people_and_jobs_prompt_3_5.format(text)}\n",
    "            ]\n",
    "        )\n",
    "    elif MODEL == \"gpt-4\":\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_people_and_jobs_prompt_4},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "        )\n",
    "    return response.choices[0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract names and roles using the desired GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_gpt_data = {}\n",
    "for url, wp_text in tqdm.tqdm(webpages_text.items()):\n",
    "    wp_text = wp_text.replace(\"\\n\", \" . \")\n",
    "    try:\n",
    "        raw_data = extract_names_and_roles(wp_text)\n",
    "        raw_gpt_data[url] = raw_data\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping long text ({e})\")\n",
    "        raw_gpt_data[url] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the extracted data in order to enforce the desired output structure, removing anything non compliant with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_gpt_data = []\n",
    "for url in webpages_text:\n",
    "    current_data = {\"url\": url, \"text\": webpages_text[url], \"people\": []}\n",
    "    for row in raw_gpt_data[url].split(\"\\n\"):\n",
    "        if \":\" not in row or \"-\" not in row:\n",
    "            continue\n",
    "        try:\n",
    "            name, role = row.split(\":\")\n",
    "        except:\n",
    "            print(row)\n",
    "            continue\n",
    "        parsed_name = name.strip(\"- \")\n",
    "        if \"?\" in name:\n",
    "            continue\n",
    "        current_data[\"people\"].append({\"name\": parsed_name, \"role\": role.strip()})\n",
    "    parsed_gpt_data.append(current_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gpt4_extracted_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(parsed_gpt_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results aggregation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_data_path = r\".\\gpt35_extracted_data.json\"\n",
    "gpt4_data_path = r\"\\gpt4_extracted_data.json\"\n",
    "gpt35_data = json.load(open(gpt35_data_path, \"r\", encoding=\"utf-8\"))\n",
    "gpt4_data = json.load(open(gpt4_data_path, \"r\", encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the first 100 entry of the dataset and get urls and people's names and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_spaziodati_people = []\n",
    "for i in range(0, 100):\n",
    "    curr_people = {\"url\": webpages_df.iloc[i].url, \"people\": []}\n",
    "    for person in webpages_df.iloc[i][\"people\"]:\n",
    "        curr_people[\"people\"].append({\n",
    "            \"name\": f\"{person['name']} {person['surname']}\",\n",
    "            \"role\": person['role']\n",
    "        })\n",
    "    cleaned_spaziodati_people.append(curr_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the extracted data into the same data structure, sorting the individual model results for each page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = []\n",
    "for spaziodati, gpt35, gpt4 in zip(cleaned_spaziodati_people, gpt35_data, gpt4_data):\n",
    "    if spaziodati[\"url\"] != gpt35[\"url\"]:\n",
    "        print(\"Different URL, skipping...\")\n",
    "        continue\n",
    "    current_data = {\"text3.5\": gpt35[\"text\"], \"text4\": gpt4[\"text\"], \"people\": {}}\n",
    "    current_data[\"people\"][\"spaziodati\"] = sorted(spaziodati[\"people\"], key=lambda x: x[\"name\"])\n",
    "    current_data[\"people\"][\"gpt3.5\"] = sorted(gpt35[\"people\"], key=lambda x: x[\"name\"])\n",
    "    current_data[\"people\"][\"gpt4\"] = sorted(gpt4[\"people\"], key=lambda x: x[\"name\"])\n",
    "    complete_data.append(current_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Removal\n",
    "Remove each duplicate entry in each page. A duplicate entry is an entry which has the same exact name and role as another extracted person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_duplicates_data = []\n",
    "duplicates = {model: 0 for model in complete_data[0][\"people\"]}\n",
    "for entry in complete_data:\n",
    "    current_general_entry = entry.copy()\n",
    "    people = current_general_entry[\"people\"]\n",
    "    current_general_entry[\"people\"] = {}\n",
    "    for model, current_model_people in people.items():\n",
    "        cleaned_model_entry = []\n",
    "        for i, person in enumerate(current_model_people):\n",
    "            if person in people[model][i+1:]:\n",
    "                duplicates[model] += 1\n",
    "                continue\n",
    "            cleaned_model_entry.append(person)\n",
    "        current_general_entry[\"people\"][model] = cleaned_model_entry\n",
    "    no_duplicates_data.append(current_general_entry)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base': 266, 'gpt3.5_new': 15, 'gpt4_op': 2, 'gpt4_full': 2}\n"
     ]
    }
   ],
   "source": [
    "complete_data = no_duplicates_data\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize \"null\" role from SpazioDati to \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in complete_data:\n",
    "    for base_person in entry[\"people\"][\"spaziodati\"]:\n",
    "        if base_person[\"spaziodati\"]:\n",
    "            continue\n",
    "        base_person[\"role\"] = \"?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate partial duplicates. A partial duplicate is an entry which shares the same name as another one in the same page, but having different roles. In this case, the two entries are merged, maintaining the name and joining the two roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_partial_duplicates_data = []\n",
    "partial_duplicates_counts = {model: 0 for model in complete_data[0][\"people\"]}\n",
    "for entry in complete_data:\n",
    "    current_entry = entry.copy()\n",
    "    current_entry[\"people\"] = {}\n",
    "    for model_name, extracted_people in entry[\"people\"].items():\n",
    "        no_partial_duplicates_model_data = {}\n",
    "        for i, person in enumerate(extracted_people):\n",
    "            person_name = person[\"name\"]\n",
    "            if person_name not in no_partial_duplicates_model_data:\n",
    "                no_partial_duplicates_model_data[person[\"name\"]] = person[\"role\"]\n",
    "            else:\n",
    "                partial_duplicates_counts[model_name] += 1\n",
    "                no_partial_duplicates_model_data[person[\"name\"]] += \", \" + person[\"role\"]\n",
    "        no_partial_duplicates_model_data = [{\"name\": name, \"role\": role} for name, role in no_partial_duplicates_model_data.items()]\n",
    "        current_entry[\"people\"][model_name] = no_partial_duplicates_model_data\n",
    "    no_partial_duplicates_data.append(current_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(partial_duplicates_counts)\n",
    "complete_data = no_partial_duplicates_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"comparison_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(complete_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"comparison_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    complete_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_people(complete_data):\n",
    "    models_count = {model: 0 for model in complete_data[0][\"people\"]}\n",
    "    for entry in complete_data:\n",
    "        complete_extracted_people = entry[\"people\"]\n",
    "        for model_name, extracted_people in complete_extracted_people.items():\n",
    "            models_count[model_name] += len(extracted_people)\n",
    "    return models_count\n",
    "\n",
    "def print_people_count(people_count):\n",
    "    print(\"Number of people extracted from 100 pages:\")\n",
    "    for model, count in people_count.items():\n",
    "        printable_model_name = model.replace(\"_\", \" \").title()\n",
    "        print(f\"{printable_model_name}: {count}\") \n",
    "\n",
    "people_counts = count_people(complete_data)\n",
    "print_people_count(people_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Match\n",
    "A match is considered *exact* when the two names or roles are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def parse_entity(entity: str) -> str:\n",
    "    entity = re.sub(r\"\\be\\b\", \"\", entity.lower())\n",
    "    return re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", entity)\n",
    "\n",
    "def get_exact_name_and_role_matches(base_people, extracted_people) -> int:\n",
    "    exact_matches = 0\n",
    "    for extracted_person in extracted_people:\n",
    "        name = parse_entity(extracted_person[\"name\"])\n",
    "        role = parse_entity(extracted_person[\"role\"])\n",
    "        exact_matches += any(\n",
    "            [\n",
    "                name == parse_entity(base_person[\"name\"]) and role == parse_entity(base_person[\"role\"]) \n",
    "                for base_person in base_people \n",
    "                if base_person[\"name\"] and base_person[\"role\"]\n",
    "            ]\n",
    "        )\n",
    "    return exact_matches\n",
    "\n",
    "def get_exact_name_matches(base_people, extracted_people) -> int:\n",
    "    exact_matches = 0\n",
    "    for extracted_person in extracted_people:\n",
    "        name = parse_entity(extracted_person[\"name\"])\n",
    "        exact_matches += any(\n",
    "            [\n",
    "                name == parse_entity(base_person[\"name\"])\n",
    "                for base_person in base_people \n",
    "                if base_person[\"name\"]\n",
    "            ]\n",
    "        )\n",
    "    return exact_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_exact_matches(complete_data, only_name_match:bool=False):\n",
    "    models_count = {model: 0 for model in complete_data[0][\"people\"] if model != \"spaziodati\"}\n",
    "    for entry in complete_data:\n",
    "        complete_extracted_people = entry[\"people\"]\n",
    "        base_people = complete_extracted_people[\"spaziodati\"]\n",
    "        for model_name, extracted_people in complete_extracted_people.items():\n",
    "            if model_name == \"spaziodati\":\n",
    "                continue\n",
    "            if only_name_match:\n",
    "                models_count[model_name] += get_exact_name_matches(base_people, extracted_people)\n",
    "            else:\n",
    "                models_count[model_name] += get_exact_name_and_role_matches(base_people, extracted_people)\n",
    "    return models_count\n",
    "\n",
    "def print_exact_matches(base_num_people: int, exact_full_matches, exact_name_matches):\n",
    "    print(\"Matches with base data\")\n",
    "    print(\"Full matches:\")\n",
    "    for model, count in exact_full_matches.items():\n",
    "        printable_model_name = model.replace(\"_\", \" \").title()\n",
    "        print(f\"\\t{printable_model_name}: {count}/{base_num_people} ({count*100/base_num_people:.2f}%)\") \n",
    "    print(\"Name matches:\")\n",
    "    for model, count in exact_name_matches.items():\n",
    "        printable_model_name = model.replace(\"_\", \" \").title()\n",
    "        print(f\"\\t{printable_model_name}: {count}/{base_num_people} ({count*100/base_num_people:.2f}%)\") \n",
    "\n",
    "exact_full_matches = count_exact_matches(complete_data)\n",
    "exact_name_matches = count_exact_matches(complete_data, only_name_match=True)\n",
    "print_exact_matches(people_counts[\"spaziodati\"], exact_full_matches, exact_name_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Match\n",
    "A match is considered *partial* when at least one of the token/words in the name or role is present in both entries. This includes a lot of false positives, but makes it possible to find same roles written in slightly different ways or same names written in different ways (surname first or name first). \n",
    "\n",
    "Examples:\n",
    "- \"Mario Rossi\" and \"Rossi Mario\"\n",
    "- \"Call center operator\" and \"Operator in a call center\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partial_tokenized_name_and_role_matches(base_people, extracted_people) -> int:\n",
    "    tokenized_partial_matches = 0\n",
    "    for person in extracted_people:\n",
    "        if not person[\"role\"]:\n",
    "            continue\n",
    "        tokenized_name = parse_entity(person[\"name\"]).split()\n",
    "        tokenized_role = parse_entity(person[\"role\"]).split()\n",
    "        for base_person in base_people:\n",
    "            if not (base_person[\"name\"] and base_person[\"role\"]):\n",
    "                continue\n",
    "            partial_name_match = any([1 for token in tokenized_name if token in parse_entity(base_person[\"name\"])])\n",
    "            partial_role_match = any([1 for token in tokenized_role if token in parse_entity(base_person[\"role\"])])\n",
    "            tokenized_partial_matches += int(partial_name_match and partial_role_match) \n",
    "            if partial_name_match and partial_role_match:\n",
    "                break\n",
    "    return tokenized_partial_matches\n",
    "\n",
    "\n",
    "def get_partial_tokenized_name_matches(base_people, extracted_people) -> int:\n",
    "    tokenized_partial_matches = 0\n",
    "    for person in extracted_people:\n",
    "        tokenized_name = parse_entity(person[\"name\"]).split()\n",
    "        for base_person in base_people:\n",
    "            if not base_person[\"name\"]:\n",
    "                continue\n",
    "            partial_name_match = any([1 for token in tokenized_name if token in parse_entity(base_person[\"name\"])])\n",
    "            tokenized_partial_matches += int(partial_name_match)\n",
    "            if partial_name_match:\n",
    "                break\n",
    "    return tokenized_partial_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_partial_matches(complete_data, only_name_match:bool=False):\n",
    "    models_count = {model: 0 for model in complete_data[0][\"people\"] if model != \"spaziodati\"}\n",
    "    for entry in complete_data:\n",
    "        complete_extracted_people = entry[\"people\"]\n",
    "        base_people = complete_extracted_people[\"spaziodati\"]\n",
    "        for model_name, extracted_people in complete_extracted_people.items():\n",
    "            if model_name == \"spaziodati\":\n",
    "                continue\n",
    "            if only_name_match:\n",
    "                models_count[model_name] += get_partial_tokenized_name_matches(base_people, extracted_people)\n",
    "            else:\n",
    "                models_count[model_name] += get_partial_tokenized_name_and_role_matches(base_people, extracted_people)\n",
    "    return models_count\n",
    "\n",
    "def print_partial_matches(base_num_people: int, partial_full_matches, partial_name_matches):\n",
    "    print(\"Partial (tokenized) matches with base data\")\n",
    "    print(\"Full matches:\")\n",
    "    for model, count in partial_full_matches.items():\n",
    "        printable_model_name = model.replace(\"_\", \" \").title()\n",
    "        print(f\"\\t{printable_model_name}: {count}/{base_num_people} ({count*100/base_num_people:.2f}%)\") \n",
    "    print(\"Name matches:\")\n",
    "    for model, count in partial_name_matches.items():\n",
    "        printable_model_name = model.replace(\"_\", \" \").title()\n",
    "        print(f\"\\t{printable_model_name}: {count}/{base_num_people} ({count*100/base_num_people:.2f}%)\") \n",
    "\n",
    "partial_full_matches = count_partial_matches(complete_data)\n",
    "partial_name_matches = count_partial_matches(complete_data, only_name_match=True)\n",
    "print_partial_matches(people_counts[\"spaziodati\"], partial_full_matches, partial_name_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing extraction stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_empty_people(all_people, type_: str) -> int:\n",
    "    return sum([1 for entry in all_people if not entry[\"people\"][type_]])\n",
    "\n",
    "def count_jobs_not_found(all_people, type_: str) -> int:\n",
    "    no_jobs = 0\n",
    "    for entry in all_people:\n",
    "        for person in entry[\"people\"][type_]:\n",
    "            if not person[\"role\"] or person[\"role\"] == \"?\":\n",
    "                no_jobs += 1\n",
    "    return no_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No people extracted\")\n",
    "print(f\"\\tSpazioDati: {count_empty_people(complete_data, 'spaziodati')}\")\n",
    "print(f\"\\tChatGPT 3.5: {count_empty_people(complete_data, 'gpt3.5')}\")\n",
    "print(f\"\\tChatGPT 4: {count_empty_people(complete_data, 'gpt4')}\")\n",
    "print(\"=========\")\n",
    "print(\"Job not extracted\")\n",
    "print(f\"\\tSpazioDati: {count_jobs_not_found(complete_data, 'spaziodati')}\")\n",
    "print(f\"\\tChatGPT 3.5: {count_jobs_not_found(complete_data, 'gpt3.5')}\")\n",
    "print(f\"\\tChatGPT 4: {count_jobs_not_found(complete_data, 'gpt4')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different number of extractions stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaziodati_more_than_gpt35 = 0 \n",
    "gpt35_more_than_spaziodati = 0 \n",
    "spaziodati_more_than_gpt4 = 0\n",
    "gpt4_more_than_spaziodati = 0\n",
    "for entry in complete_data:\n",
    "    if len(entry[\"people\"][\"spaziodati\"]) > len(entry[\"people\"][\"gpt4\"]):\n",
    "        spaziodati_more_than_gpt4 += 1\n",
    "    if len(entry[\"people\"][\"spaziodati\"]) < len(entry[\"people\"][\"gpt4\"]):\n",
    "        gpt4_more_than_spaziodati += 1\n",
    "    if len(entry[\"people\"][\"spaziodati\"]) > len(entry[\"people\"][\"gpt3.5\"]):\n",
    "        spaziodati_more_than_gpt35 = 0 \n",
    "    if len(entry[\"people\"][\"spaziodati\"]) < len(entry[\"people\"][\"gpt3.5\"]):\n",
    "        gpt35_more_than_spaziodati = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Entries where GPT4 has extracted less people than SpazioDati: {spaziodati_more_than_gpt4}\")\n",
    "print(f\"Entries where GPT4 has extracted more people than SpazioDati: {gpt4_more_than_spaziodati}\")\n",
    "print(f\"Entries where GPT3.5 has extracted less people than SpazioDati: {spaziodati_more_than_gpt35}\")\n",
    "print(f\"Entries where GPT3.5 has extracted more people than SpazioDati: {gpt35_more_than_spaziodati}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
